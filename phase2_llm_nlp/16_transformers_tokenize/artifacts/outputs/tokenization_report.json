{
  "model": "bert-base-uncased",
  "text": "Hello Yesh. Today we tokenize text using Transformers.",
  "tokens": [
    "hello",
    "yes",
    "##h",
    ".",
    "today",
    "we",
    "token",
    "##ize",
    "text",
    "using",
    "transformers",
    "."
  ],
  "input_ids": [
    101,
    7592,
    2748,
    2232,
    1012,
    2651,
    2057,
    19204,
    4697,
    3793,
    2478,
    19081,
    1012,
    102
  ],
  "attention_mask": [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1
  ],
  "token_count": 12
}